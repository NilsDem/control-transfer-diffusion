{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio-to-audio generation \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"../images/method.png\" alt=\"Example Image\" width=\"800\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook implements the inference for audio-to-audio generation. We demonstrate using the demo samples from the [webpage](https://nilsdem.github.io/control-transfer-diffusion/), but you can load your own structure and timbre targets. \n",
    "Please note that although any structure input can be used, the model require samples from the datasets (or quite similar) for the timbre target.\n",
    "\n",
    "\n",
    "Make sure to [download]() the pretrained models and place them in `./pretrained`. Two pretrained models are available, one trained on [SLAKH 2100](http://www.slakh.com/), and one trained on multiple real-world instrumental recordings (Maestro, URMP, Filobass, GuitarSet...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import paths\n",
    "folder = \"../pretrained/slakh/\"\n",
    "checkpoint_path = folder + \"checkpoint.pt\"\n",
    "config = folder + \"config.gin\"\n",
    "\n",
    "autoencoder_path = \"../pretrained/AE_slakh.pt\"\n",
    "\n",
    "# GPU\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate te model and load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.model import EDM_ADV\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = gin.query_parameter(\"%SR\")\n",
    "audio_length = gin.query_parameter(\"%X_LENGTH\")\n",
    "\n",
    "# Instantiate model\n",
    "blender = EDM_ADV()\n",
    "\n",
    "# Load checkpoints\n",
    "state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"model_state\"]\n",
    "blender.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "emb_model = torch.jit.load(autoencoder_path).eval().to(device)\n",
    "\n",
    "# Send to device\n",
    "blender = blender.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading some audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '../audios/slakh/true/piano_guitar_1.wav'\n",
    "path2 = '../audios/slakh/target/piano_guitar_1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr):\n",
    "    audio_full, sr = librosa.load(path, sr=sr)\n",
    "    audio = audio_full[:audio_length]\n",
    "    audio = torch.from_numpy(audio).reshape(1, 1, -1) / audio.max()\n",
    "    return audio\n",
    "\n",
    "\n",
    "def process_audio(audio):\n",
    "    audio = audio.to(device)\n",
    "    z = emb_model.encode(audio)\n",
    "    cqt = blender.time_transform(audio)\n",
    "    cqt = torch.nn.functional.interpolate(cqt,\n",
    "                                          size=(z.shape[-1]),\n",
    "                                          mode=\"nearest\")\n",
    "    cqt = (cqt - torch.min(cqt)) / (torch.max(cqt) - torch.min(cqt) + 1e-4)\n",
    "    return z, cqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio1, audio2 = load_audio(path1, sr=SR), load_audio(path2, sr=SR)\n",
    "\n",
    "print(\"Sample 1\")\n",
    "display(Audio(audio1.squeeze(), rate=SR))\n",
    "print(\"Sample 2\")\n",
    "display(Audio(audio2.squeeze(), rate=SR))\n",
    "\n",
    "# Compute embeddings and CQT\n",
    "z1, cqt1 = process_audio(audio1)\n",
    "z2, cqt2 = process_audio(audio2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 40  #Number of diffusion steps\n",
    "guidance = 2.0  #Classifier free guidance strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structure representation\n",
    "time_cond1, time_cond2 = blender.encoder_time(cqt1), blender.encoder_time(cqt2)\n",
    "\n",
    "# Compute timbre representation\n",
    "zsem1, zsem2 = blender.encoder(z1), blender.encoder(z2)\n",
    "\n",
    "# Sample initial noise\n",
    "x0 = torch.randn_like(z1)\n",
    "\n",
    "print(\"Timbre of sample 1 and structure of sample 2\")\n",
    "xS = blender.sample(x0,\n",
    "                    time_cond=time_cond2,\n",
    "                    zsem=zsem1,\n",
    "                    nb_step=nb_steps,\n",
    "                    guidance=guidance,\n",
    "                    guidance_type=\"time_cond\")\n",
    "\n",
    "audio_out = emb_model.decode(xS).cpu().numpy().squeeze()\n",
    "display(Audio(audio_out, rate=SR))\n",
    "\n",
    "print(\"Timbre of sample 2 and structure of sample 1\")\n",
    "xS = blender.sample(x0,\n",
    "                    time_cond=time_cond1,\n",
    "                    zsem=zsem2,\n",
    "                    nb_step=nb_steps,\n",
    "                    guidance=guidance,\n",
    "                    guidance_type=\"time_cond\")\n",
    "\n",
    "audio_out = emb_model.decode(xS).cpu().numpy().squeeze()\n",
    "display(Audio(audio_out, rate=SR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
