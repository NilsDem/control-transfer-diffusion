{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDI-to-audio Generation Notebook\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"../images/method.png\" alt=\"Example Image\" width=\"800\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook implements the inference for MIDI-to-audio generation. We demonstrate using the demo samples from the [webpage](https://nilsdem.github.io/control-transfer-diffusion/), but you can load your own midi files and timbre targets. \n",
    "Please note that although any MIDI file can be used, the model require samples from the datasets (or quite similar) for the timbre target.\n",
    "\n",
    "Make sure to [download]() the pretrained models and place them in `./pretrained`. The only checkpoint available so far was trained on the [SLAKH 2100](http://www.slakh.com/) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")  # eventually change working directory to root of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "\n",
    "gin.enter_interactive_mode()\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import paths\n",
    "folder = \"./pretrained/slakh_midi/\"\n",
    "checkpoint_path = folder + \"checkpoint.pt\"\n",
    "autoencoder_path = \"./pretrained/AE_slakh.pt\"\n",
    "config = folder + \"config.gin\"\n",
    "\n",
    "# GPU\n",
    "device = \"cuda:0\"  # or \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion.model import EDM_ADV\n",
    "\n",
    "# Parse config\n",
    "gin.parse_config_file(config)\n",
    "SR = gin.query_parameter(\"%SR\")\n",
    "audio_length = gin.query_parameter(\"%X_LENGTH\")\n",
    "\n",
    "# Instantiate model\n",
    "blender = EDM_ADV()\n",
    "\n",
    "# Load checkpoints\n",
    "state_dict = torch.load(checkpoint_path)[\"model_state\"]\n",
    "blender.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "emb_model = torch.jit.load(autoencoder_path).eval().to(device)\n",
    "\n",
    "# Send to device\n",
    "blender = blender.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading some audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path, sr):\n",
    "    audio_full, sr = librosa.load(path, sr=sr)\n",
    "    audio = audio_full[:audio_length]\n",
    "    audio = torch.from_numpy(audio).reshape(1, 1, -1) / audio.max()\n",
    "    return audio\n",
    "\n",
    "\n",
    "def process_audio(audio):\n",
    "    audio = audio.to(device)\n",
    "    z = emb_model.encode(audio)\n",
    "    return z\n",
    "\n",
    "\n",
    "def normalize(S):\n",
    "    S = (S - np.min(S)) / (np.max(S) - np.min(S) + 1e-4)\n",
    "    return S\n",
    "\n",
    "\n",
    "def get_midi(file, audio_length, comp_ratio, sr):\n",
    "    midi_data = pretty_midi.PrettyMIDI(file)\n",
    "    length = audio_length\n",
    "    tstart = 0\n",
    "    tend = length\n",
    "    midi_data.adjust_times([0, tend], [0, tend])\n",
    "\n",
    "    pr = midi_data.get_piano_roll(\n",
    "        times=np.linspace(0, audio_length / sr, audio_length // comp_ratio))\n",
    "\n",
    "    pr = normalize(pr)\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = './audios/midi/true/voice.wav'\n",
    "midi_file = \"./audios/midi/midi/voice.mid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio1 = load_audio(audio_file, sr=SR)\n",
    "\n",
    "print(\"Sample 1\")\n",
    "display(Audio(audio1.squeeze(), rate=SR))\n",
    "\n",
    "# Compute embeddings and CQT\n",
    "z1 = process_audio(audio1)\n",
    "\n",
    "# Get audio length and compression ratio\n",
    "audio_length = audio1.shape[-1]\n",
    "comp_ratio = audio_length // z1.shape[-1]\n",
    "\n",
    "# Load the midi\n",
    "\n",
    "pr = get_midi(midi_file, audio_length, comp_ratio, SR)\n",
    "plt.imshow(pr, aspect=\"auto\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_steps = 60  #Number of diffusion steps\n",
    "guidance = 2.0  #Classifier free guidance strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structure representation\n",
    "time_cond = torch.from_numpy(pr).reshape(1, pr.shape[-2],\n",
    "                                         pr.shape[-1]).float().to(device)\n",
    "\n",
    "time_cond = blender.encoder_time(time_cond)\n",
    "\n",
    "# Compute timbre representation\n",
    "zsem = blender.encoder(z1)\n",
    "\n",
    "# Sample initial noise\n",
    "x0 = torch.randn_like(z1)\n",
    "\n",
    "xS = blender.sample(x0,\n",
    "                    time_cond=time_cond,\n",
    "                    zsem=zsem,\n",
    "                    nb_step=nb_steps,\n",
    "                    guidance=guidance,\n",
    "                    guidance_type=\"time_cond\")\n",
    "\n",
    "audio_out = emb_model.decode(xS).cpu().numpy().squeeze()\n",
    "display(Audio(audio_out, rate=SR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
